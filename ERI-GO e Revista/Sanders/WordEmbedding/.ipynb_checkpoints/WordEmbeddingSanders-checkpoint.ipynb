{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessamento(object):\n",
    "    def __init__(self):\n",
    "        self.all_twitter_messages = None\n",
    "        self.polarity_tweets = None\n",
    "        self.tweets_stemming = None\n",
    "        self.palavras = []\n",
    "\n",
    "    def read_tweets_from_file(self, dataset):\n",
    "        self.all_twitter_messages = dataset['text'].values\n",
    "\n",
    "        return self.all_twitter_messages\n",
    "\n",
    "    def read_polarity_from_file(self, dataset):\n",
    "        self.polarity_tweets = dataset['class'].values\n",
    "\n",
    "        return self.polarity_tweets\n",
    "\n",
    "    def clean_tweets(self, tweet):\n",
    "        tweet = re.sub('@(\\w{1,15})\\b', '', tweet)\n",
    "        tweet = tweet.replace(\"via \", \"\")\n",
    "        tweet = tweet.replace(\"RT \", \"\")\n",
    "        tweet = tweet.lower()\n",
    "\n",
    "        return tweet\n",
    "\n",
    "    def clean_url(self, tweet):\n",
    "        tweet = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%)*\\b', '', tweet, flags=re.MULTILINE)\n",
    "        tweet = tweet.replace(\"http\", \"\")\n",
    "        tweet = tweet.replace(\"htt\", \"\")\n",
    "\n",
    "        return tweet\n",
    "\n",
    "    def remove_stop_words(self, tweet):\n",
    "        english_stops = set(stopwords.words('english'))\n",
    "\n",
    "        words = [i for i in tweet.split() if not i in english_stops]\n",
    "\n",
    "        return (\" \".join(words))\n",
    "\n",
    "    def stemming_tweets(self, tweet):\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        self.tweets_stemming = ps.stem(tweet)\n",
    "\n",
    "        return self.tweets_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>1,26418790706713E+017</th>\n",
       "      <th>2011-10-18 22:06:03</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>apple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1,26417285559763E+017</td>\n",
       "      <td>2011-10-18 22:00:04</td>\n",
       "      <td>RT @Jewelz2611 @mashable @apple, iphones r 2 e...</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>1,26416915664085E+017</td>\n",
       "      <td>2011-10-18 21:58:36</td>\n",
       "      <td>@mashable @apple, iphones r 2 expensive. Most ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>1,2641610921268E+017</td>\n",
       "      <td>2011-10-18 21:55:23</td>\n",
       "      <td>THiS IS WHAT WiLL KiLL APPLE http://t.co/72Jw4...</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>1,26411162622497E+017</td>\n",
       "      <td>2011-10-18 21:35:44</td>\n",
       "      <td>@apple why my tunes no go on my iPhone? iPhone...</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>1,26410591949697E+017</td>\n",
       "      <td>2011-10-18 21:33:28</td>\n",
       "      <td>@apple needs to hurry up and release #iTunesMatch</td>\n",
       "      <td>negative</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  1,26418790706713E+017  2011-10-18 22:06:03  \\\n",
       "0  12  1,26417285559763E+017  2011-10-18 22:00:04   \n",
       "1  13  1,26416915664085E+017  2011-10-18 21:58:36   \n",
       "2  14   1,2641610921268E+017  2011-10-18 21:55:23   \n",
       "3  20  1,26411162622497E+017  2011-10-18 21:35:44   \n",
       "4  21  1,26410591949697E+017  2011-10-18 21:33:28   \n",
       "\n",
       "                                                text     class  apple  \n",
       "0  RT @Jewelz2611 @mashable @apple, iphones r 2 e...  negative  apple  \n",
       "1  @mashable @apple, iphones r 2 expensive. Most ...  negative  apple  \n",
       "2  THiS IS WHAT WiLL KiLL APPLE http://t.co/72Jw4...  negative  apple  \n",
       "3  @apple why my tunes no go on my iPhone? iPhone...  negative  apple  \n",
       "4  @apple needs to hurry up and release #iTunesMatch  negative  apple  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre = Preprocessamento()\n",
    "\n",
    "tweets = pre.read_tweets_from_file(dataset)\n",
    "classes = pre.read_polarity_from_file(dataset)\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    tweets[i] = pre.clean_tweets(tweets[i])\n",
    "    tweets[i] = pre.clean_url(tweets[i])\n",
    "    tweets[i] = pre.remove_stop_words(tweets[i])\n",
    "    #tweets[i] = pre.stemming_tweets(tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, glove):\n",
    "        self.glove = glove\n",
    "        self.gloveweight = None\n",
    "        self.dim = len(glove.itervalues().next())\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.gloveweight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.glove[w] * self.gloveweight[w]\n",
    "                         for w in words if w in self.glove] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_glove(tweets, dim):\n",
    "    if dim == 25:\n",
    "        with open(\"glove.twitter.27B.25d.txt\", \"rb\") as lines:\n",
    "            glove = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "                for line in lines}\n",
    "\n",
    "    elif dim == 50:\n",
    "        with open(\"glove.twitter.27B.50d.txt\", \"rb\") as lines:\n",
    "            glove = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "                for line in lines}\n",
    "\n",
    "    elif dim == 100:\n",
    "        with open(\"glove.twitter.27B.100d.txt\", \"rb\") as lines:\n",
    "            glove = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "                for line in lines}\n",
    "\n",
    "    elif dim == 200:\n",
    "        with open(\"glove.twitter.27B.200d.txt\", \"rb\") as lines:\n",
    "            glove = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "                for line in lines}\n",
    "\n",
    "    else:\n",
    "        raise IOError(\"Dimens√£o do Word Embedding GloVe incorreta.\")\n",
    "\n",
    "    vec = TfidfEmbeddingVectorizer(glove)\n",
    "    vec.fit(tweets)\n",
    "    matrix = vec.transform(tweets)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_embedding = reading_glove(tweets, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74158081, -0.09020527, -0.23958878, ...,  0.41816403,\n",
       "         0.51970329,  0.54564075],\n",
       "       [ 0.75250516, -0.10413246, -0.27322124, ...,  0.41866373,\n",
       "         0.52397909,  0.54214831],\n",
       "       [ 0.59369938,  0.1585609 ,  0.0125712 , ...,  0.38925703,\n",
       "         0.3441153 ,  0.1891449 ],\n",
       "       ...,\n",
       "       [ 0.80997711,  0.06203335, -0.32969777, ...,  0.18935582,\n",
       "         0.4486442 ,  0.41699773],\n",
       "       [ 0.63714674,  0.27908308, -0.53452825, ...,  0.42038817,\n",
       "         0.36260061,  0.45314085],\n",
       "       [ 0.81551927,  0.03060585, -0.21039117, ...,  0.40729726,\n",
       "         0.44257708,  0.3831267 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando modelo Bag-of-Words a partir de features do dataset\n",
    "vec = CountVectorizer(binary=True)\n",
    "vec.fit(tweets)\n",
    "matrix_bow = vec.transform(tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 66.91\n",
      "Precision..: 43.28\n",
      "Recall.....: 34.26\n",
      "F1-Score...: 29.36\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.2778    0.0088    0.0170       570\n",
      "   negative     0.3438    0.0337    0.0614       653\n",
      "    neutral     0.6767    0.9852    0.8023      2503\n",
      "\n",
      "avg / total     0.5573    0.6691    0.5523      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO WORD EMBEDDING DE 25 DIMENS√ïES\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_embedding, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 67.39\n",
      "Precision..: 50.22\n",
      "Recall.....: 35.36\n",
      "F1-Score...: 31.52\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.3846    0.0263    0.0493       570\n",
      "   negative     0.4400    0.0505    0.0907       653\n",
      "    neutral     0.6819    0.9840    0.8056      2503\n",
      "\n",
      "avg / total     0.5940    0.6739    0.5646      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO WORD EMBEDDING DE 50 DIMENS√ïES\n",
    "matrix_embedding = reading_glove(tweets, 50)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_embedding, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 67.55\n",
      "Precision..: 52.13\n",
      "Recall.....: 36.19\n",
      "F1-Score...: 33.15\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.4130    0.0333    0.0617       570\n",
      "   negative     0.4660    0.0735    0.1270       653\n",
      "    neutral     0.6849    0.9788    0.8059      2503\n",
      "\n",
      "avg / total     0.6050    0.6755    0.5731      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO WORD EMBEDDING DE 100 DIMENS√ïES\n",
    "matrix_embedding = reading_glove(tweets, 100)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_embedding, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 67.79\n",
      "Precision..: 54.27\n",
      "Recall.....: 37.07\n",
      "F1-Score...: 34.73\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.4255    0.0351    0.0648       570\n",
      "   negative     0.5154    0.1026    0.1711       653\n",
      "    neutral     0.6872    0.9744    0.8060      2503\n",
      "\n",
      "avg / total     0.6171    0.6779    0.5814      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO WORD EMBEDDING DE 200 DIMENS√ïES\n",
    "matrix_embedding = reading_glove(tweets, 200)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_embedding, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.62\n",
      "Precision..: 74.23\n",
      "Recall.....: 60.76\n",
      "F1-Score...: 65.07\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6636    0.3807    0.4838       570\n",
      "   negative     0.7746    0.5054    0.6117       653\n",
      "    neutral     0.7888    0.9369    0.8565      2503\n",
      "\n",
      "avg / total     0.7671    0.7762    0.7566      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_bow, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_positivo = pd.read_csv('opinion_lexicon/positive-words.csv')\n",
    "lex_negativo = pd.read_csv('opinion_lexicon/negative-words.csv')\n",
    "lex_positivo = lex_positivo['pos']\n",
    "lex_negativo = lex_negativo['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           a+\n",
       "1       abound\n",
       "2      abounds\n",
       "3    abundance\n",
       "4     abundant\n",
       "Name: pos, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_positivo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2-faced\n",
       "1       2-faces\n",
       "2      abnormal\n",
       "3       abolish\n",
       "4    abominable\n",
       "Name: neg, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_negativo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6789"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lex_positivo)+len(lex_negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.43\n",
      "Precision..: 73.51\n",
      "Recall.....: 61.21\n",
      "F1-Score...: 65.25\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6737    0.3912    0.4950       570\n",
      "   negative     0.7407    0.5161    0.6083       653\n",
      "    neutral     0.7908    0.9289    0.8543      2503\n",
      "\n",
      "avg / total     0.7641    0.7743    0.7562      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 25 DIMENS√ïES - UNIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 25)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 76.76\n",
      "Precision..: 72.09\n",
      "Recall.....: 59.95\n",
      "F1-Score...: 63.85\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6398    0.3614    0.4619       570\n",
      "   negative     0.7367    0.5100    0.6027       653\n",
      "    neutral     0.7862    0.9273    0.8510      2503\n",
      "\n",
      "avg / total     0.7552    0.7676    0.7479      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 50 DIMENS√ïES - UNIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 50)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.48\n",
      "Precision..: 73.78\n",
      "Recall.....: 60.87\n",
      "F1-Score...: 65.04\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6860    0.3947    0.5011       570\n",
      "   negative     0.7370    0.4977    0.5941       653\n",
      "    neutral     0.7903    0.9337    0.8560      2503\n",
      "\n",
      "avg / total     0.7650    0.7748    0.7558      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 100 DIMENS√ïES - UNIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 100)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.62\n",
      "Precision..: 73.94\n",
      "Recall.....: 61.66\n",
      "F1-Score...: 65.79\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6705    0.4140    0.5119       570\n",
      "   negative     0.7557    0.5069    0.6068       653\n",
      "    neutral     0.7919    0.9289    0.8549      2503\n",
      "\n",
      "avg / total     0.7670    0.7762    0.7590      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 200 DIMENS√ïES - UNIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 200)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gram(object):\n",
    "    def __init__(self):\n",
    "        self.bigram = None\n",
    "        self.trigram = None\n",
    "        \n",
    "    def create_bigram(self, tweet):\n",
    "        self.bigram = []\n",
    "        \n",
    "        for i in range(len(tweet)-1):\n",
    "            b_gram = tweet[i] + \"_\" + tweet[i+1]\n",
    "            self.bigram.append(b_gram)\n",
    "            \n",
    "        return (\" \".join(self.bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram = Gram()\n",
    "\n",
    "tweets_bigram = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    tweets_bigram.append(gram.create_bigram(tweets[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3726"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_uni_big = tweets + tweets_bigram\n",
    "tweets_uni_big[0]\n",
    "len(tweets_uni_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_vectorizer(tweets):\n",
    "    # Criando modelo Bag-of-Words a partir de features do dataset\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    cv.fit(tweets)\n",
    "    matrix = cv.transform(tweets).toarray()\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_bow_big = count_vectorizer(tweets_bigram)\n",
    "matrix_bow_uni_big = count_vectorizer(tweets_uni_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 75.17\n",
      "Precision..: 74.40\n",
      "Recall.....: 53.88\n",
      "F1-Score...: 58.49\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7292    0.3070    0.4321       570\n",
      "   negative     0.7492    0.3522    0.4792       653\n",
      "    neutral     0.7537    0.9573    0.8434      2503\n",
      "\n",
      "avg / total     0.7492    0.7517    0.7166      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS - BIGRAM\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_bow_big, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.59\n",
      "Precision..: 75.26\n",
      "Recall.....: 60.37\n",
      "F1-Score...: 64.93\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7115    0.3807    0.4960       570\n",
      "   negative     0.7619    0.4900    0.5965       653\n",
      "    neutral     0.7844    0.9405    0.8554      2503\n",
      "\n",
      "avg / total     0.7693    0.7759    0.7550      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS - UNIGRAM + BIGRAM\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix_bow_uni_big, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.59\n",
      "Precision..: 75.44\n",
      "Recall.....: 60.22\n",
      "F1-Score...: 64.82\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7176    0.3789    0.4960       570\n",
      "   negative     0.7620    0.4855    0.5931       653\n",
      "    neutral     0.7836    0.9421    0.8556      2503\n",
      "\n",
      "avg / total     0.7698    0.7759    0.7546      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 25 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 25)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 78.02\n",
      "Precision..: 75.88\n",
      "Recall.....: 61.20\n",
      "F1-Score...: 65.82\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7169    0.4088    0.5207       570\n",
      "   negative     0.7713    0.4855    0.5959       653\n",
      "    neutral     0.7883    0.9417    0.8582      2503\n",
      "\n",
      "avg / total     0.7744    0.7802    0.7606      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 50 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 50)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 77.94\n",
      "Precision..: 75.74\n",
      "Recall.....: 60.79\n",
      "F1-Score...: 65.43\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7000    0.3930    0.5034       570\n",
      "   negative     0.7852    0.4870    0.6011       653\n",
      "    neutral     0.7871    0.9437    0.8583      2503\n",
      "\n",
      "avg / total     0.7734    0.7794    0.7589      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 100 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 100)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 78.34\n",
      "Precision..: 76.16\n",
      "Recall.....: 61.91\n",
      "F1-Score...: 66.50\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7333    0.4246    0.5378       570\n",
      "   negative     0.7589    0.4916    0.5967       653\n",
      "    neutral     0.7925    0.9413    0.8605      2503\n",
      "\n",
      "avg / total     0.7775    0.7834    0.7649      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + WORD EMBEDDING DE 200 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 200)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.00000 0.00000 1.00000',\n",
       " '0.00000 0.00000 1.00000',\n",
       " '0.00000 1.00000 0.00000',\n",
       " '0.00000 1.00000 0.00000',\n",
       " '0.00000 0.00000 1.00000']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex_file = open('lex.txt', 'r')\n",
    "lexicon = lex_file.read().splitlines()\n",
    "lex_file.close()\n",
    "lexicon[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = lexicon[0].split()\n",
    "lista = map(float, lista)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_lex = []\n",
    "\n",
    "for i in range(len(lexicon)):\n",
    "    vec = lexicon[i].split()\n",
    "    vec = map(float, vec)\n",
    "    matrix_lex.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 0.0],\n",
       " [0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_lex[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 78.77\n",
      "Precision..: 73.69\n",
      "Recall.....: 65.66\n",
      "F1-Score...: 68.81\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6464    0.4842    0.5537       570\n",
      "   negative     0.7480    0.5727    0.6487       653\n",
      "    neutral     0.8164    0.9129    0.8619      2503\n",
      "\n",
      "avg / total     0.7784    0.7877    0.7774      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + DICION√ÅRIO L√âXICO - UNIGRAM + BIGRAM\n",
    "lr = LogisticRegression(C=10.0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_lex), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 78.56\n",
      "Precision..: 74.53\n",
      "Recall.....: 63.36\n",
      "F1-Score...: 67.25\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6893    0.4281    0.5281       570\n",
      "   negative     0.7421    0.5421    0.6265       653\n",
      "    neutral     0.8045    0.9305    0.8629      2503\n",
      "\n",
      "avg / total     0.7759    0.7856    0.7703      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + L√âXICO + WORD EMBEDDING DE 25 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 25)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding, matrix_lex), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 78.77\n",
      "Precision..: 75.17\n",
      "Recall.....: 63.68\n",
      "F1-Score...: 67.66\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.7003    0.4263    0.5300       570\n",
      "   negative     0.7505    0.5528    0.6367       653\n",
      "    neutral     0.8043    0.9313    0.8632      2503\n",
      "\n",
      "avg / total     0.7790    0.7877    0.7725      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + L√âXICO + WORD EMBEDDING DE 50 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 50)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding, matrix_lex), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 79.15\n",
      "Precision..: 75.83\n",
      "Recall.....: 64.15\n",
      "F1-Score...: 68.26\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6975    0.4491    0.5464       570\n",
      "   negative     0.7707    0.5406    0.6355       653\n",
      "    neutral     0.8066    0.9349    0.8660      2503\n",
      "\n",
      "avg / total     0.7836    0.7915    0.7767      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + L√âXICO + WORD EMBEDDING DE 100 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 100)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding, matrix_lex), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia...: 79.36\n",
      "Precision..: 75.82\n",
      "Recall.....: 64.89\n",
      "F1-Score...: 68.85\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive     0.6919    0.4649    0.5561       570\n",
      "   negative     0.7720    0.5498    0.6422       653\n",
      "    neutral     0.8106    0.9321    0.8671      2503\n",
      "\n",
      "avg / total     0.7857    0.7936    0.7801      3726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREDI√á√ÉO COM MODELO BAG-OF-WORDS + L√âXICO + WORD EMBEDDING DE 200 DIMENS√ïES - UNIGRAM + BIGRAM\n",
    "matrix_embedding = reading_glove(tweets, 200)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "matrix = np.concatenate((matrix_bow_uni_big, matrix_embedding, matrix_lex), axis=1)\n",
    "\n",
    "resultados = cross_val_predict(lr, matrix, classes, cv = kfold)\n",
    "\n",
    "sentimento = ['positive', 'negative', 'neutral']\n",
    "\n",
    "print(\"Acur√°cia...: %.2f\" %(metrics.accuracy_score(classes,resultados) * 100))\n",
    "print(\"Precision..: %.2f\" %(metrics.precision_score(classes,resultados,average='macro') * 100))\n",
    "print(\"Recall.....: %.2f\" %(metrics.recall_score(classes,resultados, average='macro') * 100))\n",
    "print(\"F1-Score...: %.2f\" %(metrics.f1_score(classes,resultados, average='macro') * 100))\n",
    "#print()\n",
    "print(metrics.classification_report(classes,resultados,sentimento,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_positivo = list(lex_positivo)\n",
    "lex_negativo = list(lex_negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(vocabulary=set(lex_positivo+lex_negativo), binary=True)\n",
    "cv2.fit(tweets_uni_big)\n",
    "bow_lex = cv2.transform(tweets_uni_big).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3726, 6786)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_lex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acuracia = [78.56, 78.77, 79.15, 79.36]\n",
    "precisao = [74.53, 75.17, 75.83, 75.82]\n",
    "recall = [63.36, 63.68, 64.15, 64.89]\n",
    "f1 = [67.25, 67.66, 68.26, 68.85]\n",
    "dimensoes = [25, 50, 100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(dimensoes, acuracia)\n",
    "ax.plot(dimensoes, precisao)\n",
    "ax.plot(dimensoes, recall)\n",
    "ax.plot(dimensoes, f1)\n",
    "ax.set_title(\"'fivethirtyeight' style sheet\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
